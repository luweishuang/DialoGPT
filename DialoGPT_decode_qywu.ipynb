{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DialoGPT Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EnUCcmutUj8",
        "colab_type": "code",
        "outputId": "8c95657c-66ce-43f5-e532-3f5e2ce55ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov 19 09:39:53 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KmHwJqptSnO",
        "colab_type": "code",
        "outputId": "1f028986-8180-4ea8-b6c1-18f7257bbc94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install gdown"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\r\u001b[K     |█                               | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\r\u001b[K     |▍                               | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 23.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 27.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 30.2MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 31.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 33.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 71kB 34.0MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 32.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 34.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102kB 35.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112kB 35.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 143kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 153kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 174kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 184kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 194kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 204kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 215kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 225kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 235kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 245kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 256kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 266kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 276kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 286kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 296kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 307kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 317kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 327kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 337kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 348kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 358kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 368kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 378kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 389kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 399kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 409kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 419kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 430kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 440kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 450kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 460kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 471kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 481kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 491kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 501kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 512kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 522kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 532kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 542kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 552kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 563kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 573kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 583kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 593kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 604kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 614kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 624kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 634kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 645kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 655kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 665kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 675kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 686kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 696kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 706kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 716kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 727kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 737kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 747kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 757kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 768kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 778kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 788kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 798kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 808kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 819kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 829kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 839kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 849kB 35.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 860kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 40.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.17 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.17)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.17->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.17->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=5d5eeb1b8ffff82022d92e63242c7952f4ee1631ae29d1d454ab3b158637a09c\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, regex, sentencepiece, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.9.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzIGSWmQtjJC",
        "colab_type": "code",
        "outputId": "225340c3-de23-45a6-94bc-2762e17210f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# import huggingface transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW, WarmupLinearSchedule"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYKxjlBTt0wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
        "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
        "                whose total probability mass is greater than or equal to the threshold top_p.\n",
        "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
        "                the threshold top_p.\n",
        "    \"\"\"\n",
        "    # batch support!\n",
        "    if top_k > 0:\n",
        "        values, _ = torch.topk(logits, top_k)\n",
        "        min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])\n",
        "        logits = torch.where(logits < min_values, \n",
        "                             torch.ones_like(logits, dtype=logits.dtype) * -float('Inf'), \n",
        "                             logits)\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        \n",
        "        sorted_logits = sorted_logits.masked_fill_(sorted_indices_to_remove, filter_value)\n",
        "        logits = torch.zeros_like(logits).scatter(1, sorted_indices, sorted_logits)\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qoz-KEcbWgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed=42\n",
        "np.random.seed(seed)\n",
        "torch.random.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0cQrCEXubmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2_small_config = GPT2Config()\n",
        "gpt2_medium_config = GPT2Config(n_ctx=1024, n_embd=1024, n_layer=24, n_head=16)\n",
        "gpt2_large_config = GPT2Config(n_ctx=1024, n_embd=1280, n_layer=36, n_head=20)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lagx89vDt4dR",
        "colab_type": "code",
        "outputId": "bf612ecf-06c7-43ff-cc30-f11960e04e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:01<00:00, 945379.77B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 508787.62B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LqRxx9xwsdj",
        "colab_type": "code",
        "outputId": "6a023e0e-02f2-474b-e54c-a8eb61b8a44e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# download all model weights\n",
        "!wget https://convaisharables.blob.core.windows.net/lsp/multiref/small_ft.pkl\n",
        "!wget https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
        "!wget https://convaisharables.blob.core.windows.net/lsp/multiref/large_ft.pkl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-19 09:40:47--  https://convaisharables.blob.core.windows.net/lsp/multiref/small_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 351265273 (335M) [application/octet-stream]\n",
            "Saving to: ‘small_ft.pkl’\n",
            "\n",
            "small_ft.pkl        100%[===================>] 334.99M  8.71MB/s    in 35s     \n",
            "\n",
            "2019-11-19 09:41:23 (9.55 MB/s) - ‘small_ft.pkl’ saved [351265273/351265273]\n",
            "\n",
            "--2019-11-19 09:41:24--  https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862954531 (823M) [application/octet-stream]\n",
            "Saving to: ‘medium_ft.pkl’\n",
            "\n",
            "medium_ft.pkl       100%[===================>] 822.98M  8.87MB/s    in 1m 41s  \n",
            "\n",
            "2019-11-19 09:43:06 (8.13 MB/s) - ‘medium_ft.pkl’ saved [862954531/862954531]\n",
            "\n",
            "--2019-11-19 09:43:08--  https://convaisharables.blob.core.windows.net/lsp/multiref/large_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1752291179 (1.6G) [application/octet-stream]\n",
            "Saving to: ‘large_ft.pkl’\n",
            "\n",
            "large_ft.pkl         77%[==============>     ]   1.27G  11.9MB/s    eta 40s    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KzZLsMwt6Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model\n",
        "model_size = \"medium\"\n",
        "\n",
        "if model_size == \"small\":\n",
        "    model = GPT2LMHeadModel(gpt2_small_config)\n",
        "    model.load_state_dict(torch.load(\"small_ft.pkl\"), strict=False)\n",
        "elif model_size == \"medium\":\n",
        "    model = GPT2LMHeadModel(gpt2_medium_config)\n",
        "    model.load_state_dict(torch.load(\"medium_ft.pkl\"), strict=False)\n",
        "elif model_size == \"large\":\n",
        "    model = GPT2LMHeadModel(gpt2_large_config)\n",
        "    model.load_state_dict(torch.load(\"large_ft.pkl\"), strict=False)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_UH3o1kbTGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jAVF1rpyCKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# beg huggingface not to change this anymore\n",
        "model.lm_head.weight.data = model.transformer.wte.weight.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lecxn-gwMpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eos = [tokenizer.encoder[\"<|endoftext|>\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIojaBXEwYeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "past = None\n",
        "temperature = 0.9\n",
        "top_k = -1\n",
        "top_p = 0.9\n",
        "\n",
        "model.eval()\n",
        "prev_input = None\n",
        "\n",
        "while True:\n",
        "    with torch.no_grad():\n",
        "        # input and update B's utterance\n",
        "        user = input(\"User:\")\n",
        "        \n",
        "        if user == \"quit\":\n",
        "            \"stop talking!\"\n",
        "            break\n",
        "        \n",
        "        user = tokenizer.encode(user)\n",
        "        prev_input = user\n",
        "        prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)\n",
        "        _, past = model(prev_input, past=past)\n",
        "\n",
        "        prev_input = torch.LongTensor([eos]).to(device)\n",
        "    \n",
        "\n",
        "        sent = []\n",
        "        for i in range(500):\n",
        "            logits, past = model(prev_input, past=past)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            prev_input = torch.multinomial(probs, num_samples=1)\n",
        "            prev_word = prev_input.item()\n",
        "\n",
        "            if prev_word == eos[0]:\n",
        "                break\n",
        "            sent.append(prev_word)\n",
        "        \n",
        "        print(\"Bot:\", tokenizer.decode(sent))\n",
        "        prev_input = torch.LongTensor([eos]).to(device)\n",
        "        _, past = model(prev_input, past=past)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4TeIw7ypL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}